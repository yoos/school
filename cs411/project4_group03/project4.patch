--- linux/mm/slob.c	2014-04-10 13:24:55.659062946 -0700
+++ project2/mm/slob.c	2014-06-02 21:49:17.124324015 -0700
@@ -187,6 +187,30 @@
 	int size;
 };
 
+/* Toggle value to switch between original and best fit */
+#define BEST_FIT
+
+/* global to keep tack fo the amount of memory actually in use */
+static unsigned long memory_used = 0;
+
+/* global to keep track of the amount of memory that is claimed */
+static unsigned long memory_claimed = 0;
+
+/* global to keep track of search loop iterations */
+static unsigned long alloc_loop_iters = 0;
+
+/* gloab to keep track of allocations attempted */
+static unsigned long alloc_count = 0;
+
+struct page_fit {
+	struct slob_page *sp;
+
+	slob_t *cur;
+	slob_t *prev;
+
+	int delta;
+};
+
 /*
  * slob_lock protects all slob allocator structures.
  */
@@ -315,17 +339,122 @@
 	}
 }
 
+#ifdef BEST_FIT
+static void *slob_page_alloc_best(struct page_fit *fit, size_t size, int align)
+{
+	struct slob_page *sp = fit->sp;
+
+	slob_t *next, *aligned = NULL;
+	int delta = 0, units = SLOB_UNITS(size);
+
+	slob_t *cur = fit->cur;
+	slob_t *prev = fit->prev;
+
+	slobidx_t avail = slob_units(cur);
+
+	if (align) {
+		aligned = (slob_t *)ALIGN((unsigned long)cur, align);
+		delta = aligned - cur;
+	}
+
+	if (delta) {
+		next = slob_next(cur);
+		set_slob(aligned, avail - delta, next);
+		set_slob(cur, delta, aligned);
+		prev = cur;
+		cur = aligned; 
+		avail = slob_units(cur);
+	}
+
+	next = slob_next(cur);
+	if (avail == units) { /*exact fit? unlink */
+		if (prev)
+			set_slob(prev, slob_units(prev), next);
+		else
+			sp->free = next;
+	} else { /* fragment*/
+		if (prev)
+			set_slob(prev, slob_units(prev), cur + units);
+		else
+			sp->free = cur + units;
+		set_slob(cur + units, avail - units, next);
+	}
+
+	sp->units -= units;
+	if (!sp->units)
+		clear_slob_page_free(sp);
+
+	return cur;
+}
+#endif
+
+/*
+ * Find the best fit for the memory that is being allocated. If a spot 
+ * is found on the page that can hold the memory size requested and the amount
+ * of left over space is smaller than what was previously found, which is 
+ * contained in the best pointer, then the best pointer is updated, and 1
+ * is returned to indicate that the page is the best fit found thus far.
+ */
+#ifdef BEST_FIT
+static void slob_best_fit(struct slob_page *sp, struct page_fit *best,
+                          size_t size, int align)
+{
+	slob_t *prev, *cur, *aligned = NULL;
+	int delta = 0, units = SLOB_UNITS(size);
+
+	/* loop through each slob */
+	for (prev = NULL, cur = sp->free; ; prev = cur, cur = slob_next(cur)) {
+		/* get the space available */
+		slobidx_t avail = slob_units(cur);
+
+		/* adjust delta based on alignment */
+		if (align) {
+			aligned = (slob_t *)ALIGN((unsigned long)cur, align);
+			delta = aligned - cur;
+		}
+	
+		/* update best fit if necessary */
+		if (avail >= units+delta && avail-units-delta < best->delta) {
+			best->sp = sp;
+
+			best->cur = cur;
+			best->prev = prev;
+			best->delta = avail - units - delta;
+		}
+
+		/* break out of the loop if we hit the last slob */
+		if (slob_last(cur))
+			break;
+
+		}
+}
+#endif
+
 /*
  * slob_alloc: entry point into the slob allocator.
  */
 static void *slob_alloc(size_t size, gfp_t gfp, int align, int node)
 {
 	struct slob_page *sp;
-	struct list_head *prev;
+
 	struct list_head *slob_list;
+	struct list_head *prev = NULL;
+
 	slob_t *b = NULL;
 	unsigned long flags;
 
+#ifdef BEST_FIT
+	int first_fit = 0;
+
+	struct page_fit fit;
+	fit.sp = NULL;
+
+	fit.prev = NULL;
+	fit.cur = NULL;
+	fit.delta = 9999999;
+#endif
+
+	alloc_count++;
 	if (size < SLOB_BREAK1)
 		slob_list = &free_slob_small;
 	else if (size < SLOB_BREAK2)
@@ -336,6 +465,7 @@
 	spin_lock_irqsave(&slob_lock, flags);
 	/* Iterate through each partially free page, try to find room */
 	list_for_each_entry(sp, slob_list, list) {
+	alloc_loop_iters++;
 #ifdef CONFIG_NUMA
 		/*
 		 * If there's a node specification, search for a partial
@@ -348,6 +478,33 @@
 		if (sp->units < SLOB_UNITS(size))
 			continue;
 
+#ifdef BEST_FIT
+	/* check for a new best fit */
+	slob_best_fit(sp, &fit, size, align);
+	if (fit.sp != NULL && first_fit == 0)
+		first_fit = fit.delta;
+
+	prev = sp->list.prev;
+
+	/* if we found an exact match, no need to keep searching */
+	if (fit.delta == 0)
+		break;
+
+	}
+
+	/* if we found a fitted page, attempt the allocation */
+	if (fit.sp != NULL) {
+		printk(KERN_INFO "FIRST FIT DELTA: %d\n", first_fit);
+		printk(KERN_INFO "BEST FIT DELTA: %d\n\n", fit.delta);
+		
+		if ((b = slob_page_alloc_best(&fit, size, align)) &&
+		    prev != slob_list->prev &&
+		    slob_list->next != prev->next)
+			list_move_tail(slob_list, prev->next);
+	}
+#else
+	
+
 		/* Attempt to alloc */
 		prev = sp->list.prev;
 		b = slob_page_alloc(sp, size, align);
@@ -362,6 +519,7 @@
 			list_move_tail(slob_list, prev->next);
 		break;
 	}
+#endif
 	spin_unlock_irqrestore(&slob_lock, flags);
 
 	/* Not enough space: must allocate a new page */
@@ -369,6 +527,10 @@
 		b = slob_new_pages(gfp & ~__GFP_ZERO, 0, node);
 		if (!b)
 			return NULL;
+		
+		/* add the newly allocated page to the total claimed */
+		memory_claimed += PAGE_SIZE;
+
 		sp = slob_page(b);
 		set_slob_page(sp);
 
@@ -384,6 +546,10 @@
 	}
 	if (unlikely((gfp & __GFP_ZERO) && b))
 		memset(b, 0, size);
+
+	/* add the size we just allocated to the total in use */
+	memory_used += size;
+
 	return b;
 }
 
@@ -407,7 +573,13 @@
 
 	spin_lock_irqsave(&slob_lock, flags);
 
+	/* update the memory used to reflect us freeing some memory */
+	memory_used -= size;
+
 	if (sp->units + units == SLOB_UNITS(PAGE_SIZE)) {
+		/* we are freeing a whole page so update the memory claimed */
+		memory_claimed -= PAGE_SIZE;
+
 		/* Go directly to page allocator. Do not pass slob allocator */
 		if (slob_page_free(sp))
 			clear_slob_page_free(sp);
@@ -688,3 +860,29 @@
 {
 	/* Nothing to do */
 }
+
+/*
+ * System calls that will allow us to get an idea of the amount
+ * of fragmentation suffered by the algorithm. The ratio of the memory 
+ * we are actually using to the memory that we are actually claiming
+ * should give us an idea of how much fragmentation we have.
+ */
+asmlinkage unsigned long sys_get_slob_claimed(void)
+{
+	return memory_claimed;
+}
+
+asmlinkage unsigned long sys_get_slob_used(void)
+{
+	return memory_used;
+}
+
+asmlinkage unsigned long sys_get_slob_alloc_count(void)
+{
+	return alloc_count;
+}
+
+asmlinkage unsigned long sys_get_slob_search_count(void)
+{
+	return alloc_loop_iters;
+}
